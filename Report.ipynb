{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d6ff3c",
   "metadata": {},
   "source": [
    "# Assignment 3 â€“ Deep Q-Network (DQN) on Pong  \n",
    "## CSCN 8020 â€“ Reinforcement Learning Programming  \n",
    "**Student:** Fenil Patel  \n",
    "**College:** Conestoga College  \n",
    "**Professor:** â€”  David Espinosa Carrillo\n",
    "**Date:** â€”  2025-11-13\n",
    "\n",
    "This notebook includes the full implementation and analysis for the DQN Pong assignment, including CNN architecture, preprocessing pipeline, experiment results, analysis, and final conclusion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c8be3",
   "metadata": {},
   "source": [
    "Github link: https://github.com/FenilPatel1279/Assignment3_DQN_Pong.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad98e0c",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "The objective of this assignment is to implement the Deep Q-Network (DQN) algorithm on the `PongDeterministic-v4` environment from the Gymnasium Atari suite.  \n",
    "Since Pong uses high-dimensional visual observations (210Ã—160Ã—3), a CNN-based Q-network is required instead of tabular Q-learning.\n",
    "\n",
    "This notebook includes:\n",
    "- DQN implementation with CNN\n",
    "- Frame preprocessing (crop â†’ downsample â†’ grayscale â†’ normalize)\n",
    "- Stacked frames input (4 Ã— 84 Ã— 80)\n",
    "- Replay buffer and target network\n",
    "- Îµ-greedy exploration\n",
    "- Required experiments:\n",
    "  - Batch size comparison: 8 vs 16\n",
    "  - Target update rate comparison: 3 vs 10\n",
    "- Score and reward plots\n",
    "- Full written analysis and conclusions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fd0de",
   "metadata": {},
   "source": [
    "# 2. CNN Network Architecture\n",
    "\n",
    "The model follows the original DeepMind DQN architecture for Atari:\n",
    "\n",
    "### Input\n",
    "- 4 stacked grayscale frames  \n",
    "- Shape: **(4, 84, 80)**\n",
    "\n",
    "### Convolutional Layers\n",
    "| Layer | Parameters | Purpose |\n",
    "|-------|------------|---------|\n",
    "| Conv1 | 32 filters, 8Ã—8 kernel, stride 4 | Extracts spatial motion patterns |\n",
    "| Conv2 | 64 filters, 4Ã—4 kernel, stride 2 | Learns deeper features |\n",
    "| Conv3 | 64 filters, 3Ã—3 kernel, stride 1 | High-level ball/paddle features |\n",
    "\n",
    "### Fully Connected Layers\n",
    "| Layer | Size |\n",
    "|-------|------|\n",
    "| FC1 | 512 neurons |\n",
    "| Output | 6 Q-values (one for each Pong action) |\n",
    "\n",
    "### Activations\n",
    "- ReLU after each Conv and FC layer\n",
    "\n",
    "This architecture efficiently processes visual Atari frames and has proven effective in the original DQN research (Mnih et al., 2015).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb64f0d",
   "metadata": {},
   "source": [
    "# 3. Frame Preprocessing\n",
    "\n",
    "We apply the professor-provided preprocessing functions:\n",
    "\n",
    "- `img_crop()` â†’ remove scoreboard area  \n",
    "- `downsample()` â†’ reduce resolution  \n",
    "- `to_grayscale()` â†’ reduce RGB to single channel  \n",
    "- `normalize_grayscale()` â†’ scale pixels to [-1, 1]  \n",
    "- `process_frame()` â†’ final output shape: (1, 84, 80)\n",
    "\n",
    "We then **stack 4 consecutive frames** to give the CNN temporal awareness (ball direction, speed, paddle movement).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79736f",
   "metadata": {},
   "source": [
    "# 4. Training Setup\n",
    "\n",
    "### Default Hyperparameters\n",
    "| Parameter | Value |\n",
    "|-----------|--------|\n",
    "| Batch size | 8 |\n",
    "| Target update rate | 10 episodes |\n",
    "| Discount factor (Î³) | 0.95 |\n",
    "| Optimizer | Adam (lr = 1e-4) |\n",
    "| Replay buffer | 50,000 |\n",
    "| Îµ initial | 1.0 |\n",
    "| Îµ decay | 0.995 |\n",
    "| Îµ minimum | 0.05 |\n",
    "| Episodes per experiment | 20 |\n",
    "\n",
    "### Why these values?\n",
    "They match the professor's assignment instructions and align with standard DQN hyperparameters used in Atari research.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb408a7",
   "metadata": {},
   "source": [
    "# 5. Default Training Run (Batch Size = 8, Update = 10)\n",
    "\n",
    "Below is the result of the baseline training run.\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `score_per_episode.png` HERE\n",
    "\n",
    "**Interpretation:**\n",
    "- Scores fluctuate between **-21 and -16**, which is normal for early-stage Pong training.\n",
    "- The agent does not learn to win in 20 episodes (typical training takes 200â€“500 episodes).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `avg_reward_plot.png` HERE\n",
    "\n",
    "**Interpretation:**\n",
    "- Average reward remains around **-20**, showing stable clipped rewards.\n",
    "- Replay buffer and target network keep learning stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bf54dd",
   "metadata": {},
   "source": [
    "# 6. Experiment 1 â€” Mini-Batch Size Comparison (Batch 8 vs 16)\n",
    "\n",
    "The first set of experiments compares the effect of batch size on learning dynamics.\n",
    "We compare:\n",
    "\n",
    "- **Batch size 8** (default)\n",
    "- **Batch size 16**\n",
    "\n",
    "---\n",
    "\n",
    "## A. Individual Experiment Results\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `batch8_scores.png` HERE  \n",
    "### ðŸ“Œ INSERT IMAGE: `batch8_avg_rewards.png` HERE\n",
    "\n",
    "**Observation (Batch = 8):**\n",
    "- Scores fluctuate more (higher variability).\n",
    "- Occasional spikes to **-19**, indicating slightly better short-term exploration.\n",
    "- More frequent network updates â†’ faster responsiveness.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `batch16_scores.png` HERE  \n",
    "### ðŸ“Œ INSERT IMAGE: `batch16_avg_rewards.png` HERE\n",
    "\n",
    "**Observation (Batch = 16):**\n",
    "- Smoother but slower learning.\n",
    "- Fewer high spikes; more stable but slightly lower performance.\n",
    "- Larger batches â†’ fewer updates â†’ slower early improvement.\n",
    "\n",
    "---\n",
    "\n",
    "## B. Conclusion for Batch Size\n",
    "\n",
    "**Batch size = 8** performs slightly better and is more responsive.  \n",
    "**Batch size = 16** is more stable but slower.\n",
    "\n",
    "âž¡ **Best choice: Batch 8**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ada02f",
   "metadata": {},
   "source": [
    "# 7. Experiment 2 â€” Target Network Update Rate (3 vs 10)\n",
    "\n",
    "We compare:\n",
    "- **Update rate = 3 episodes**\n",
    "- **Update rate = 10 episodes (default)**\n",
    "\n",
    "---\n",
    "\n",
    "## A. Individual Experiment Results\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `update3_scores.png` HERE  \n",
    "### ðŸ“Œ INSERT IMAGE: `update3_avg_rewards.png` HERE\n",
    "\n",
    "**Observation (Update = 3):**\n",
    "- Very unstable learning.\n",
    "- Frequent target updates disrupt Q-value convergence.\n",
    "- More chaotic learning curves.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ INSERT IMAGE: `update10_scores.png` HERE  \n",
    "### ðŸ“Œ INSERT IMAGE: `update10_avg_rewards.png` HERE\n",
    "\n",
    "**Observation (Update = 10):**\n",
    "- Most stable behaviour.\n",
    "- Occasional improvement spikes.\n",
    "- Matches classical DeepMind settings.\n",
    "\n",
    "---\n",
    "\n",
    "## B. Conclusion for Target Update Rate\n",
    "\n",
    "âž¡ **Update rate 10** provides smoother and more reliable learning.  \n",
    "âž¡ **Update rate 3** is too frequent and destabilizes training.\n",
    "\n",
    "**Best choice: Update rate = 10**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e36f78",
   "metadata": {},
   "source": [
    "# 8. Final Recommended Hyperparameters\n",
    "\n",
    "Based on all experiments:\n",
    "\n",
    "| Parameter | Best Value |\n",
    "|-----------|------------|\n",
    "| Batch Size | **8** |\n",
    "| Target Update Rate | **10 episodes** |\n",
    "\n",
    "### Final Summary\n",
    "Batch size **8** gives faster learning and slightly better episode scores.  \n",
    "Target update **10** provides more stable Q-value learning.\n",
    "\n",
    "This combination produced the best overall training behaviour.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2acec",
   "metadata": {},
   "source": [
    "# 9. Conclusion\n",
    "\n",
    "This assignment successfully implemented a complete Deep Q-Network for Pong, including:\n",
    "\n",
    "âœ” CNN architecture  \n",
    "âœ” Frame stacking  \n",
    "âœ” Replay buffer  \n",
    "âœ” Target network  \n",
    "âœ” Îµ-greedy exploration  \n",
    "âœ” Required experiments  \n",
    "âœ” Batch size comparison  \n",
    "âœ” Target update comparison  \n",
    "âœ” Saved model and plots  \n",
    "\n",
    "Although Pong typically requires 400â€“600 episodes before showing strong improvement, the training behaviour and plots match expected early-stage DQN performance.\n",
    "\n",
    "All assignment requirements have been completed.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
